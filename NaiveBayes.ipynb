{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc416063",
   "metadata": {},
   "source": [
    "# Skills lab: Conditional Probabilities\n",
    "#### Sentiment analysis of tweets with Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3616a",
   "metadata": {},
   "source": [
    "The idea of summarizing and visualizing events and emotions of the entire world received a lot of attention in the Machine Learning community. As an example see how the [GDELT project](https://www.gdeltproject.org/) tried to convert world media and news into visualizations of the events and emotions: [LINK](https://firstmonday.org/ojs/index.php/fm/article/view/4366/3654) \n",
    "\n",
    "Our goal in this assignment is less extreme: we want to create a system for monitoring emotions of Twitter users and plot the positivity score for small geographic areas on Google maps.\n",
    "\n",
    "To classify each tweet we are going to use the Naive Bayes classifier, which, despite its “naive” assumptions, has great performance in classifying documents based on their word content.\n",
    "\n",
    "The main idea of the Document classification with Naive Bayes is as following:\n",
    "\n",
    "For each new document with words $Word_1 … Word_n$ and each category $c$ compute the probability:\n",
    "\n",
    "$P(Category = c | Word1 = true, …, Wordn = true) =\\alpha*P(Category = c) \\prod_{i=1}^n P(Wordi = true | Category = c)$\n",
    "\n",
    "Then a new document can be classified as belonging to a category $c_{best}$ for which the above probability is the highest:\n",
    "\n",
    "$c_{best} = argmax(c \\in C) P(Category = c | Word_1 = true, …, Word_n = true)$ \n",
    "\n",
    "Note, that we don't calculate any real probabilities here. Instead, we are estimating which class is more likely, given the evidence (words). This is another reason why Naïve Bayes is so robust: It is not so much interested in the real probabilities, but only determines which class is more likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a5d67",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Our training dataset was obtained from the Niek Sanders corpus, who has done an awesome job of **manually** labeling more than 5,000 tweets with 4 categories: Positive, Negative, Neutral, and Irrelevant. In general, you could use his script, [install.py](https://github.com/zfz/twitter_corpus.git), to fetch the corresponding Twitter text by tweet id. Because the script is playing nicely with Twitter's servers, it might take quite some time to download the data, so the dataset [full-corpus.csv](https://drive.google.com/file/d/1n3XFcUp8yoLf4BOj0rxEpTw1VCwyK1s-/view?usp=sharing) has been already pre-created for you by your hard-working instructor:). \n",
    "\n",
    "We also preprocessed the dataset, leaving only 2 relevant columns: the text of a twit, and the sentiment class. The preprocessed dataset is in file [labeled_corpus.tsv](https://drive.google.com/file/d/1NXvKTuTO6oNjonKkDm2DGdN5fqj-2RE7/view?usp=drive_link), and the tweet text is separated from the class label using tab (‘\\t’). That is why the file extension is tsv - which means tab-separated values (similar to csv - comma-separated values). This is our training dataset, used to estimate the probabilities of words given a category. Download the dataset [labeled_corpus.tsv](https://drive.google.com/file/d/1NXvKTuTO6oNjonKkDm2DGdN5fqj-2RE7/view?usp=drive_link), and place it in the same directory where your notebook is.\n",
    "\n",
    "Let's open the dataset and look at the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2f8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is\n",
      "positive\n",
      "\n",
      "@Apple will be adding more carrier support to the iPhone 4S (just announced)\n",
      "positive\n",
      "\n",
      "Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love affair! http://t.co/8ExbnQjY\n",
      "positive\n",
      "\n",
      "Why is #Google advertising Adwords Express on #Bing? Hmmm, I know Ballmer is happy :-D. #Google, your already a monopoly.\n",
      "negative\n",
      "\n",
      "@RIM you made it too easy for me to switch to @Apple iPhone. See ya!\n",
      "positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"labeled_corpus.tsv\", encoding=\"utf-8\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "    countdown = 5\n",
    "    \n",
    "    for row in readCSV:\n",
    "        line_arr = list(row)\n",
    "\n",
    "        tweet = line_arr[0]\n",
    "        sentiment = line_arr[1]        \n",
    "                 \n",
    "        print (tweet)\n",
    "        print (sentiment)\n",
    "        print()\n",
    "        \n",
    "        countdown-=1\n",
    "        if countdown==0: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd44b83",
   "metadata": {},
   "source": [
    "We see that some tweets are labeled as 'positive' and others as 'negative'. There are overall four different categories of documents (tweets) in this dataset. Let's store these categories in a set. We can also make use of categories as numbers or as indexes in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a671d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'positive', 'negative', 'neutral', 'irrelevant'}\n",
    "category_to_num = {\"positive\": 0, \"negative\":1, \"neutral\":2, \"irrelevant\": 3}\n",
    "num_to_category = {0: \"positive\", 1: \"negative\", 2: \"neutral\", 3: \"irrelevant\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fec21",
   "metadata": {},
   "source": [
    "## Cleaning tweets\n",
    "The main characteristic of a single tweet is that unlike other documents, the tweet is short (limited by 140 characters) and contains a lot of shortcuts and emoticons. We need to carefully preserve every single word, unlike in normal natural language documents, where we would remove some words (stop words). It is probably wise to remove words starting with @ as they represent user names. It is also desirable to strip the hashtag from the word it is attached to.\n",
    "\n",
    "To take into account all abbreviations and emoticons, we replace the most obvious of them with the words which may help us with sentiment analysis. The range of frequent emoticons and their replacement is defined in emo_repl dictionary below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb57548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",  # :D in lower case\n",
    "    \":dd\": \" good \", # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d6601",
   "metadata": {},
   "source": [
    "Here are also the most common abbreviations to be replaced with actual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd94829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions module\n",
    "\n",
    "# Define abbreviations\n",
    "# as regular expressions together with their expansions\n",
    "# (\\b marks the word boundary):\n",
    "re_repl = {\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2eb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that e.g. :dd is replaced before :d\n",
    "emo_repl_order = [k for (k_len,k) in\n",
    "                  reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d4af6",
   "metadata": {},
   "source": [
    "And here is a function that will apply all of the above to a single tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c066e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    for k in emo_repl_order:\n",
    "        tweet = tweet.replace(k, emo_repl[k])\n",
    "\n",
    "    for r, repl in re_repl.items():\n",
    "        tweet = re.sub(r, repl, tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040af0e",
   "metadata": {},
   "source": [
    "## Parsing and counting words\n",
    "To work with a tweet as a document, we first need to obtain separate words from a tweet text. \n",
    "\n",
    "Because the same word rarely occurs in a tweet, we are going to account for the presence/absence of a word rather than the count of words per tweet. \n",
    "\n",
    "**Stop and think:** words with which prefix should you ignore when parsing a tweet? (#, @?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e354e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 1. Count words in each category</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4eb30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def split_tweet(string):\n",
    "    split = string.split()\n",
    "    output = []\n",
    "    for w in split:\n",
    "        if '#' not in w and '@' not in w:\n",
    "            str = \"\"\n",
    "            for ch in w:\n",
    "                if (ch not in exclude):\n",
    "                    str += ch\n",
    "            output += [str]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19e0fc",
   "metadata": {},
   "source": [
    "The probabilities of  $P(Word_i = true | Category = c)$ can be easily computed from the dataset as the number of documents in category $c$ which contain this word / total number of documents in this category (conditional probability). $P(Category=c)$ is simply the number of all documents in this category divided by the total number of documents in the dataset.\n",
    "\n",
    "Based on all the information given above, read the dataset, clean each tweet, remove some prefixed words, and split the tweet into words using the function `split_tweet()` defined above.\n",
    "\n",
    "Then for each word from an obtained list generate the following counts: \n",
    "- Number of tweets in category \"positive\" that contain this word  \n",
    "- Number of tweets in category \"negative\" that contain this word  \n",
    "- Number of tweets in category \"neutral\" that contain this word   \n",
    "- Number of tweets in category \"irrelevant\" that contain this word   \n",
    "\n",
    "Also count the total number of tweets in each category and the overall total number of tweets.\n",
    "\n",
    "Implement this inside the routine called `produce_counts`. Add counts to the corresponding entries of `word_counts_dict` and `total_entries`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57960897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_counts():\n",
    "    global word_counts_dict, total_entries\n",
    "    \n",
    "    # Initialize word_counts_dict and total_entries\n",
    "    word_counts_dict = {}\n",
    "    total_entries = [0, 0, 0, 0, 0]  # [positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "    # Read the labeled corpus\n",
    "    with open(\"labeled_corpus.tsv\", encoding=\"utf-8\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "    \n",
    "        # Iterate through each row in the labeled corpus\n",
    "        for row in readCSV:\n",
    "            tweet = row[0]\n",
    "            sentiment = row[1]\n",
    "\n",
    "            # Clean the tweet and split into words\n",
    "            cleaned_tweet = clean_tweet(tweet)\n",
    "            words = split_tweet(cleaned_tweet)\n",
    "            \n",
    "            # Update total_entries\n",
    "            total_entries[category_to_num[sentiment]] += 1  # Increment total count for the corresponding sentiment\n",
    "            \n",
    "            # Keep track of the word counts in this tweet\n",
    "            word_counts_in_tweet = {}\n",
    "            \n",
    "            # Iterate through each word in the tweet\n",
    "            for word in words:\n",
    "                if word:\n",
    "                    # Ensure that the word is not empty\n",
    "                    if word not in word_counts_dict:\n",
    "                        # Initialize counts for this word if it's not already in word_counts_dict\n",
    "                        word_counts_dict[word] = [0, 0, 0, 0, 0]  # [positive, negative, neutral, irrelevant, total counts]\n",
    "                    # Increment count for the corresponding sentiment category\n",
    "                    word_counts_dict[word][category_to_num[sentiment]] += 1\n",
    "                    # Increment the word count for this tweet\n",
    "                    if word not in word_counts_in_tweet:\n",
    "                        word_counts_in_tweet[word] = 1\n",
    "                    else:\n",
    "                        word_counts_in_tweet[word] += 1\n",
    "            \n",
    "            # Update counts for each word in this tweet based on its occurrence count\n",
    "            for word, count in word_counts_in_tweet.items():\n",
    "                word_counts_dict[word][-1] += count  # Update total count for the word\n",
    "                \n",
    "        # Update total count for the dataset\n",
    "        total_entries[-1] = sum(total_entries[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97251456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_counts():\n",
    "    global word_counts_dict, total_entries\n",
    "    \n",
    "    # Initialize variables\n",
    "    word_counts_dict = {}\n",
    "    total_entries = [0, 0, 0, 0, 0]  # [positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "    # Read the file in\n",
    "    with open(\"labeled_corpus.tsv\", encoding=\"utf-8\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "    \n",
    "        # Iterate through each row in the file\n",
    "        for row in readCSV:\n",
    "            # The tweet is in the first row, category of the tweet in the second\n",
    "            tweet = row[0]\n",
    "            category = row[1]\n",
    "\n",
    "            # Clean tweet and then split the tweet into it's words\n",
    "            tweet = clean_tweet(tweet)\n",
    "            words = split_tweet(tweet)\n",
    "            \n",
    "            # Update total_entries by changing category to number of total entries and increment\n",
    "            total_entries[category_to_num[category]] += 1  # Increment total count for the category\n",
    "            \n",
    "            # Each word in tweet is saved in a set\n",
    "            wordCounts = {}\n",
    "            \n",
    "            # Iterate through each word in the tweet\n",
    "            for word in words:\n",
    "                if word:\n",
    "                    # Ensure that the word is not empty\n",
    "                    if word not in word_counts_dict:\n",
    "                        # Initialize counts for this word if it's not already in word_counts_dict\n",
    "                        word_counts_dict[word] = [0, 0, 0, 0, 0]  # [positive, negative, neutral, irrelevant, total counts]\n",
    "                    # Increment count for the corresponding sentiment category\n",
    "                    word_counts_dict[word][category_to_num[category]] += 1\n",
    "                    # Increment the word count for this tweet\n",
    "                    if word not in wordCounts:\n",
    "                        wordCounts[word] = 1\n",
    "                    else:\n",
    "                        wordCounts[word] += 1\n",
    "            \n",
    "            # Update counts for each word in this tweet based on its occurrence count\n",
    "            for word, count in wordCounts.items():\n",
    "                word_counts_dict[word][-1] += count  # Update total count for the word\n",
    "                \n",
    "        # Update total count for the dataset\n",
    "        total_entries[-1] = sum(total_entries[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d7971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positives = 519\n",
      "total negatives = 572\n",
      "total documents = 5107\n",
      "[6, 8, 12, 0, 26]\n",
      "[12, 6, 16, 0, 34]\n"
     ]
    }
   ],
   "source": [
    "word_counts_dict = {} # \"word\": [positive, negative, neutral, irrelevant, total counts]\n",
    "total_entries = [0, 0, 0, 0, 0] #[positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "produce_counts()\n",
    "\n",
    "##### Double-check that your counts are correct\n",
    "# total positives = 519\n",
    "print (\"total positives =\", total_entries[0])\n",
    "\n",
    "# total negatives = 572\n",
    "print (\"total negatives =\", total_entries[1])\n",
    "\n",
    "# total documents = 5107\n",
    "print (\"total documents =\", total_entries[-1])\n",
    "\n",
    "#'made': [6, 8, 12, 0, 26]\n",
    "print(word_counts_dict['made'])\n",
    "#'thank': [12, 6, 16, 0, 34]\n",
    "print(word_counts_dict['thank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf90ad",
   "metadata": {},
   "source": [
    "## Build Naive Bayes\n",
    "\n",
    "It is time to build a classifier - that is to precompute: for each category $c$ the $P(Category = c)$, and for each $Word_i$ the value of  $P(Word_i|c)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2313dbd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 2. Compute probabilities</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba20c2",
   "metadata": {},
   "source": [
    "Store the probability for each word in a dictionary `probabilities_dict` where the key is the word itself, and the value is a list of probabilities of this word occuring in 4 different categories. Store prior probabilities of $P(c)$ in `prior_probabilities` list with a separate entry for each category. \n",
    "\n",
    "Use counts obtained in the previous section and implement the `probabilities()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities():\n",
    "    global prior_probabilities, probabilities_dict\n",
    "    \n",
    "    # Calculate prior probabilities P(C)\n",
    "    total_documents = total_entries[-1]\n",
    "    for i in range(4):\n",
    "        prior_probabilities[i] = total_entries[i] / total_documents\n",
    "    \n",
    "    for word, counts in word_counts_dict.items():\n",
    "        probabilities_dict[word] = []\n",
    "        for j in range(4):\n",
    "            # Calculate conditional probability for each category\n",
    "            if total_entries[j] > 0:\n",
    "                probabilities_dict[word].append(counts[j] / total_entries[j])\n",
    "            else:\n",
    "                probabilities_dict[word].append(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd706",
   "metadata": {},
   "source": [
    "Now run the function and double-check that your probabilities are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5977f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('positive') 0.10162522028588213\n",
      "P('negative') 0.11200313295476796\n",
      "P('neutral') 0.4558449187389857\n",
      "P('irrelevant') 0.3305267280203642\n",
      "[0.011560693641618497, 0.013986013986013986, 0.005154639175257732, 0.0]\n",
      "[0.023121387283236993, 0.01048951048951049, 0.006872852233676976, 0.0]\n"
     ]
    }
   ],
   "source": [
    "prior_probabilities = [0,0,0,0] #probabilities of 'positive', 'negative', 'neutral', 'irrelevant'\n",
    "probabilities_dict = {} #key: word, value array [positive, negative, neutral, irrelevant]\n",
    "\n",
    "probabilities()\n",
    "\n",
    "print(\"P('positive')\", prior_probabilities[0])     # 0.10162522028588213\n",
    "print(\"P('negative')\", prior_probabilities[1])     # 0.11200313295476796\n",
    "print(\"P('neutral')\", prior_probabilities[2])      # 0.4558449187389857\n",
    "print(\"P('irrelevant')\", prior_probabilities[3])   # 0.3305267280203642 \n",
    "\n",
    "print(probabilities_dict['made'])   #[0.011560693641618497, 0.013986013986013986, 0.005154639175257732, 0.0]\n",
    "print(probabilities_dict['thank'])  #[0.023121387283236993, 0.01048951048951049, 0.006872852233676976, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dc365",
   "metadata": {},
   "source": [
    "## Classify geo-tagged tweets\n",
    "\n",
    "The second dataset we are using in this lab is the dataset of geo-tagged tweets containing the latitude and the longitude of an author: [geodata.txt](https://drive.google.com/file/d/1u6fmbjCdsP9AA7z1tkYfLVSMFlDFW-nJ/view?usp=sharing). \n",
    "\n",
    "This dataset comes from the paper: “A Latent Variable Model for Geographic Lexical Variation”  \n",
    "by Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, and Eric P. Xing,  \n",
    "In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, 2010.\n",
    " \n",
    "For this dataset, we want to predict a category label \n",
    "of each tweet using probabilities computed in the previous step. \n",
    "\n",
    "We cannot use this dataset directly -- we need to redistribute data points into small rectangular areas of the map.\n",
    "This should be done because Google maps API would not be able to handle all our data points due to the memory and processing power constraints of the browser. Instead, we assign each data point to a small rectangular area of 0.05 x 0.05 degrees in size, and then we compute a single score for each such area to plot the center of the area as a representative of several data points. Note that this step is already completed for you: in the [geo-twits_squares.csv](https://drive.google.com/file/d/1JDEUPGqxGvt9_r8YfQ1yZM_VuQawhHEf/view?usp=sharing) the latitude and longitude represent the upper left corner of a rectangular area to which each original tweet in geo_data.txt belongs. You can download *geo-twits_squares.csv* and place it into the same directory where you store your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4785c37",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 3. Classify geo-tweets</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49645a67",
   "metadata": {},
   "source": [
    "For each twit in the [geo_twits_squares.tsv](https://drive.google.com/file/d/1JDEUPGqxGvt9_r8YfQ1yZM_VuQawhHEf/view?usp=sharing), compute probabilities of it belonging to one of 4 categories - positive, negative, neutral or irrelevant - and choose the category which is most probable. Perform the same cleaning and parsing procedures as before before computing these classes.\n",
    "\n",
    "Because the formula for classification contains a lengthy sequence of multiplications, and the multiplied probabilities are all smaller than 1, to prevent a possible numerical underflow,  we should use the log of probability instead, and compare the logs (the logs will be negative, but we still can compare them):\n",
    "\n",
    "$c_{best} = argmax(c \\in C) [log P(Category = c) + \\sum_{i=1}^n log P(Word_i = true | Category = c)]$\n",
    "\n",
    "At the end of this step, you should have a file **locations_classified.tsv**, where for each tweet you keep 3 columns: latitude, longitude, and the most probable class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177a73c",
   "metadata": {},
   "source": [
    "It might take some time to run the classifier, so you might consider using the `tqdm` Python library which will show the progress, but you do not have to. AN example of its usage is shown in the cell below (this cell is not runnable)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a908164f",
   "metadata": {},
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(readTSV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92841f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "def classify():\n",
    "    # Read in the file\n",
    "    # Had to change the encoing type due to issues that arose (and ignore the errors)\n",
    "    with open('geo_data.txt', 'r', encoding='windows-1254', errors='ignore') as file:\n",
    "        file = csv.reader(file, delimiter='\\t')  # Use tab delimiter\n",
    "        \n",
    "        # Initialize list where classified tweets will go\n",
    "        classified_tweets = []\n",
    "        \n",
    "        # For each row in the file\n",
    "        for row in file:\n",
    "            try:\n",
    "                # Place lat, long and tweet in seperate variables\n",
    "                lat = row[3]  # Latitude in 4th column\n",
    "                long = row[4]  # Longitude in 5th column\n",
    "                tweet = row[5] # Tweet in words in 6th column\n",
    "\n",
    "                # Clean the tweet and split into words\n",
    "                cleaned_tweet = clean_tweet(tweet)\n",
    "                words = split_tweet(cleaned_tweet)\n",
    "\n",
    "                # Initialize variables\n",
    "                max_log_prob = -math.inf\n",
    "                most_probable_label = None\n",
    "\n",
    "                # Get probability for each category\n",
    "                for category in ['positive', 'negative', 'neutral', 'irrelevant']:\n",
    "                    # Log of prior probability\n",
    "                    log_prob = math.log(prior_probabilities[category_to_num[category]])  \n",
    "                    # For each word in the tweet, compute the probability for each category\n",
    "                    for word in words:\n",
    "                        # If the word is in the probability dictionary (the dictionary that has each word along with\n",
    "                        # the prob that it is each category)\n",
    "                        if word in probabilities_dict:\n",
    "                            word_probs = probabilities_dict[word]\n",
    "                            # Check if probability is positive because cant take log of negative number\n",
    "                            if word_probs[category_to_num[category]] > 0:  \n",
    "                                log_prob += math.log(word_probs[category_to_num[category]])\n",
    "\n",
    "                    # Compare each value to choose the maximum probability (signifies what is most probable)\n",
    "                    if log_prob > max_log_prob:\n",
    "                        max_log_prob = log_prob\n",
    "                        most_probable_label = category\n",
    "\n",
    "                # Store calculations and lat/long in list\n",
    "                classified_tweets.append((lat, long, most_probable_label))\n",
    "                \n",
    "                # Store same results in the result array\n",
    "                result.append([lat, long, category_to_num[most_probable_label]])\n",
    "            # If unable to read in the row, throw error\n",
    "            except IndexError:\n",
    "                print(\"Skipping row due to index out of bounds:\", row)\n",
    "\n",
    "    # Write to new file (encoded differently than the read in file?)\n",
    "    with open('locations_classified.tsv', 'w', newline='', encoding='utf-8') as file:\n",
    "        # Use tabs to seperate the columns\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        # Write the headers\n",
    "        writer.writerow(['Latitude', 'Longitude', 'Class Label'])\n",
    "        # Write each row in classified_tweets\n",
    "        writer.writerows(classified_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09080a",
   "metadata": {},
   "source": [
    "Now you can run this classification routine, which might take some time to finish. \n",
    "\n",
    "If for each tweet you also add the result \\[lat, long, class\\] to the results array then the first 5 entries in this array should look like:\n",
    "`[['40.2', '-74.85', 3], ['40.2', '-74.85', 1], ['40.2', '-74.85', 0], ['40.2', '-74.85', 2], ['40.2', '-74.85', 3]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05f03a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['47.528139', '-122.197916', 1], ['47.528139', '-122.197916', 0], ['47.528139', '-122.197916', 1], ['47.528139', '-122.197916', 0], ['47.528139', '-122.197916', 3]]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "classify()    \n",
    "print(result[:5])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0103e",
   "metadata": {},
   "source": [
    "## Positivity score\n",
    "Here is where some creativity is needed.\n",
    "\n",
    "Now that we can count the number of total tweets per area, the number of positive twits and the number of negative twits in it, how would you assign a single positivity score to each area?\n",
    "\n",
    "The goal is to have a single score which reflects both the proportion of positive and the proportion of negative tweets, and we want this score to range from 0 to 1.  We want the score >=0.5 to indicate an overall positive spirit of the area, and the score < 0.5 its negative spirit.\n",
    "\n",
    "Here is one possible suggestion:\n",
    "\n",
    "Consider the following example: \n",
    "There are a total of 10 tweets in the area.\n",
    "5 of them are positive, 3 are negative, 2 irrelevant/neutral.\n",
    "Positivity = total_positive/total - total_negative/total = 5/10 - 2/10 = 3/10 > 0\n",
    "\n",
    "Let’s say in another square:\n",
    "2 are positive, 7 are negative, and total is 10.\n",
    "Positivity = 2/10 - 7/10 = -5/10 < 0.\n",
    "\n",
    "Because the positivity score defined above varies from -1 to 1, and we want it to vary from 0 to 1, we could just add 1 and divide by 2 - and we get the score in the desired interval.\n",
    "\n",
    "You can think of a different score, or you can make changes in the visualization code to accommodate the different score interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce8672",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 4. Compute positivity scores</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5a9d",
   "metadata": {},
   "source": [
    "For each rectangular geographic area compute a single positivity score.\n",
    "\n",
    "The output from this step is a dataset where for each area we have its left upper corner (latitude, longitude) and the average positivity score. You may store this dataset in file **positivity_score.tsv**. For us to be able to test your work, please  print several scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "79868cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Create variable that can hold location as key and various counts for different categories\n",
    "location_tweet_counts = {}\n",
    "\n",
    "def positivity_score():\n",
    "  \n",
    "    \n",
    "    with open('locations_classified.tsv', 'r', newline='', encoding='utf-8') as file:\n",
    "        # File is separated by tabs\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        # Skip line with headers\n",
    "        next(reader)\n",
    "        \n",
    "        # Iterate through each row\n",
    "        for row in reader:\n",
    "            lat, long, classifier = row\n",
    "            # Make the key the lat/long values for all tweets in the same area\n",
    "            key = (lat, long)\n",
    "            if key not in location_tweet_counts:\n",
    "                # Place key in list and initialize counts\n",
    "                location_tweet_counts[key] = {'total': 0, 'positive': 0, 'negative': 0, 'neutral': 0, 'irrelevant': 0}\n",
    "            # Add each same lat/long value to increment the total for future calculations\n",
    "            location_tweet_counts[key]['total'] += 1\n",
    "            # Count the classifications for each area, using the lat/long value as the key\n",
    "            if classifier == 'positive':\n",
    "                location_tweet_counts[key]['positive'] += 1\n",
    "            elif classifier == 'negative':\n",
    "                location_tweet_counts[key]['negative'] += 1\n",
    "            elif classifier == 'neutral':\n",
    "                location_tweet_counts[key]['neutral'] += 1\n",
    "            else:\n",
    "                location_tweet_counts[key]['irrelevant'] += 1\n",
    "                    \n",
    "    # Create place for the scores to be saved\n",
    "    location_pos_scores = {}\n",
    "        \n",
    "    # Iterate through each key and find calculations\n",
    "    for location, count in location_tweet_counts.items():\n",
    "        total_tweets = count['total']\n",
    "        pos_tweets = count['positive']\n",
    "        neg_tweets = count['negative']\n",
    "        \n",
    "        # Calculate positivity score\n",
    "        positivity_score = (pos_tweets - neg_tweets) / total_tweets\n",
    "        adjusted_score = (positivity_score + 1) / 2\n",
    "        location_pos_scores[location] = adjusted_score\n",
    "        \n",
    "    # Write to positivity_score.tsv\n",
    "    with open('positivity_score.tsv', 'w', newline='', encoding='utf-8') as output:\n",
    "        writer = csv.writer(output, delimiter='\\t')\n",
    "        writer.writerow(['Latitude', 'Longitude', 'Positivity Score'])  # Write header\n",
    "        \n",
    "        for location, score in location_pos_scores.items():\n",
    "            lat, long = location\n",
    "            writer.writerow([lat, long, score])\n",
    "\n",
    "    return location_pos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11fc2859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude\tLongitude\tPositivity Score\n",
      "47.528139\t-122.197916\t0.5602409638554217\n",
      "40.2015\t-74.806535\t0.5\n",
      "40.221968\t-74.734795\t0.2857142857142857\n",
      "40.221333\t-74.732688\t0.5833333333333334\n",
      "40.220681\t-74.758761\t0.0\n",
      "40.194523\t-74.756427\t0.0\n",
      "40.289891\t-74.678256\t0.5\n",
      "40.314751\t-74.659931\t0.5\n",
      "40.266259\t-74.792365\t0.0\n"
     ]
    }
   ],
   "source": [
    "location_pos_scores = positivity_score()\n",
    "\n",
    "# Open the file for reading\n",
    "with open('positivity_score.tsv', 'r', newline='', encoding='utf-8') as file:\n",
    "    # Print the first 10 lines\n",
    "    for i, line in enumerate(file):\n",
    "        # Remove whitespace\n",
    "        print(line.strip())\n",
    "        # Stop printing after first 10 iterations\n",
    "        if i == 9:\n",
    "            break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e5306",
   "metadata": {},
   "source": [
    "## Visualizing results\n",
    "Now your goal is to visualize positivity on the map. You can learn how to create your own map visualizations [here](). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596307dd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:yellow;\">\n",
    "    <h3>Task 5. Create visualization input</h3>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46694f",
   "metadata": {},
   "source": [
    "If you want to use the provided demo template, you would need to create a javascript variable `data` in JSON format and write this object to file `data.js` (located in public_html folder) as following:\n",
    "\n",
    "`var data =[{\"score\": 0, \"g\": -122.197916, \"t\": 47.528139}, {\"score\": 0.5, \"g\": -122.197916, \"t\": 47.528139}, … ];`\n",
    "\n",
    "As you see, `data` is an array of Javascript ‘objects’, and each object contains 3 attributes: ‘score’ of the area, ‘g’ for longitude and ‘t’ for latitude. Do not forget to add 0.05/2 to each coordinate to represent the center of the rectangular area rather than its left upper corner.\n",
    "\n",
    "Dump your resulting object to file `data.js`, place this file into the public_html folder, and explore the visualization by opening index.html in your browser. \n",
    "\n",
    "What areas are most positive, which ones are most negative?  Write down your observations in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52379ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_js_data(location_pos_scores):\n",
    "    # Convert scores to JavaScript format\n",
    "    # Create variable\n",
    "    data = []\n",
    "    # Seperate as given above\n",
    "    for location, score in location_pos_scores.items():\n",
    "        lat, long = location\n",
    "        # Adjust lat and long values\n",
    "        lat = float(lat) + 0.05 / 2  \n",
    "        long = float(long) + 0.05 / 2  \n",
    "        # Append data to appropriate location in data\n",
    "        data.append({'score': score, 'g': long, 't': lat})\n",
    "\n",
    "        \n",
    "    # Write JavaScript object to file\n",
    "    with open('public_html/data.js', 'w') as output_file:\n",
    "        output_file.write('var data = ' + json.dumps(data) + ';')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65216f19",
   "metadata": {},
   "source": [
    "Run it and test if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3573984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_js_data(location_pos_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4d900",
   "metadata": {},
   "source": [
    "While this does work and the file is created as told above, when I go to open the visualization created, it tells me there is a problem and it can't open. If you can assist me in finding out how to do this, I can complete this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c3984",
   "metadata": {},
   "source": [
    "## Before you submit\n",
    "Check that everything still works as expected if you call all your functions one after another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cfc15c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_js_data() missing 1 required positional argument: 'location_pos_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m classify() \u001b[38;5;66;03m#classify and write locations_classified.tsv\u001b[39;00m\n\u001b[1;32m     11\u001b[0m positivity_score() \u001b[38;5;66;03m#calculate positivity scores and write positivity_score.tsv\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m generate_js_data()\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_js_data() missing 1 required positional argument: 'location_pos_scores'"
     ]
    }
   ],
   "source": [
    "word_counts_dict = {} # \"word\": [positive, negative, neutral, irrelevant, total counts]\n",
    "total_entries = [0, 0, 0, 0, 0] #[positive, negative, neutral, irrelevant, total counts]\n",
    "\n",
    "produce_counts() #count tweets in training data\n",
    "\n",
    "prior_probabilities = [0,0,0,0] #probabilities of 'positive', 'negative', 'neutral', 'irrelevant'\n",
    "probabilities_dict = {} #key: word, value array [positive, negative, neutral, irrelevant]\n",
    "\n",
    "probabilities() #calculate probabilities\n",
    "classify() #classify and write locations_classified.tsv\n",
    "positivity_score() #calculate positivity scores and write positivity_score.tsv\n",
    "generate_js_data() #write data.js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd46f2",
   "metadata": {},
   "source": [
    "## Submit your work by committing the forked repository to github. \n",
    "*Do not commit any data files: they are too big for github*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a388f8",
   "metadata": {},
   "source": [
    "### This is the end of the Skill lab 3. \n",
    "\n",
    "Copyright &copy; 2024 Marina Barsky."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
